defaults:
  - base_target_llm
  - _self_

llm_params:
  model_name: "llama3-8b-chat"
  checkpoint: './model-cache/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/' # or replace with local DIR
prompt_manager:
  prompt_template:
    - key: system_message
      msg: "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n"
    - key: full_instruct
      msg: "{full_instruct}"  # loaded from context
    - key: separator
      msg: "<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
    - key: target
      msg: "{target}"  # loaded from context